{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    word = word[0]\n",
    "    hyponyms = list(set([w for s in word.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    \n",
    "    return hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    word = word[0]\n",
    "    hypernyms = list(set([w for s in word.closure(lambda s:s.hypernyms()) for w in s.lemma_names()]))\n",
    "    \n",
    "    return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    synonyms = list(set([w for s in word for w in s.lemma_names()]))\n",
    "    \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_tree']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms('apple_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word1, word2):\n",
    "    \n",
    "    output = np.zeros(3)\n",
    "    \n",
    "    # check hyper/hypo-nym and synonyms\n",
    "    synonyms = get_synonyms(word1)\n",
    "    hypers = get_hypernyms(word1)\n",
    "    hypos = get_hyponyms(word1)\n",
    "    \n",
    "    if word2 in synonyms:\n",
    "        output[0] = 1\n",
    "    if word2 in hypers:\n",
    "        output[1] = 1\n",
    "    if word2 in hypos:\n",
    "        output[2] = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector('bad', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.tokenization import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "from torchnlp.datasets import snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = snli_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Concept(object):\n",
    "    \"\"\"Cencept relation between words.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2rel = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word, other=None, vec=None):\n",
    "        if not other:\n",
    "            self.word2rel[word] = np.zeros(3)\n",
    "        else:\n",
    "            if not word in self.word2rel:\n",
    "                self.word2rel[word] = {other: vec}\n",
    "                self.idx += 1\n",
    "            else:\n",
    "                self.word2rel[word][other] = vec\n",
    "\n",
    "    def __call__(self, word, other):\n",
    "        if not word in self.word2rel:\n",
    "            return self.word2rel['<unk>']\n",
    "        return self.word2rel[word][other]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2rel)\n",
    "    \n",
    "def prepare_vocab(dataset, threshold):\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    for t in tqdm(dataset, desc='loading data...'):\n",
    "        premise = t['premise']\n",
    "        hypothesis = t['hypothesis']\n",
    "        premise_tokens = nltk.word_tokenize(premise)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "        tokens = premise_tokens + hypothesis_tokens\n",
    "        counter.update(tokens)\n",
    "           \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "        \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Concept()\n",
    "    vocab.add_word('<unk>')\n",
    "    \n",
    "    # Add the words to the vocabulary.\n",
    "    pbar = tqdm(total=len(words)**2)\n",
    "    for word in words:\n",
    "        for other in words:\n",
    "            pbar.update(1)\n",
    "            vec = get_vector(word, other)\n",
    "            vocab.add_word(word, other, vec)\n",
    "    pbar.close()\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading data...: 100%|██████████| 550152/550152 [02:44<00:00, 3545.55it/s]\n",
      "  4%|▎         | 651661/17665209 [49:00<3524:40:54,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "vocab = prepare_vocab(train_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare wordnet relation vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
