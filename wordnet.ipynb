{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    word = word[0]\n",
    "    hyponyms = list(set([w for s in word.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    \n",
    "    return hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypernyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    word = word[0]\n",
    "    hypernyms = list(set([w for s in word.closure(lambda s:s.hypernyms()) for w in s.lemma_names()]))\n",
    "    \n",
    "    return hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    word = wn.synsets(word)\n",
    "    if len(word) == 0:\n",
    "        return []\n",
    "    synonyms = list(set([w for s in word for w in s.lemma_names()]))\n",
    "    \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_tree']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms('apple_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word1, word2):\n",
    "    \n",
    "    output = np.zeros(3)\n",
    "    \n",
    "    # check hyper/hypo-nym and synonyms\n",
    "    synonyms = get_synonyms(word1)\n",
    "    hypers = get_hypernyms(word1)\n",
    "    hypos = get_hyponyms(word1)\n",
    "    \n",
    "    if word2 in synonyms:\n",
    "        output[0] = 1\n",
    "    if word2 in hypers:\n",
    "        output[1] = 1\n",
    "    if word2 in hypos:\n",
    "        output[2] = 1\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector('bad', 'good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.tokenization import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "from torchnlp.datasets import snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = snli_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Cencept(object):\n",
    "    \"\"\"Cencept relation between words.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2rel = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word, other=None, vec=None):\n",
    "        if not other:\n",
    "            self.word2rel[word] = np.zeros(3)\n",
    "        else:\n",
    "            if not word in self.word2rel:\n",
    "                self.word2rel[word] = {other: vec}\n",
    "                self.idx += 1\n",
    "            else:\n",
    "                self.word2rel[word][other] = vec\n",
    "\n",
    "    def __call__(self, word, other):\n",
    "        if not word in self.word2rel:\n",
    "            return self.word2rel['<unk>']\n",
    "        return self.word2rel[word][other]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2rel)\n",
    "    \n",
    "def prepare_vocab(dataset, threshold):\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    for t in tqdm(dataset):\n",
    "        \n",
    "        premise = t['premise']\n",
    "        hypothesis = t['hypothesis']\n",
    "        premise_tokens = nltk.word_tokenize(premise)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "        tokens = premise_tokens + hypothesis_tokens\n",
    "        counter.update(tokens)\n",
    "           \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "        \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Concept()\n",
    "    vocab.add_word('<unk>')\n",
    "    \n",
    "    # Add the words to the vocabulary.\n",
    "    for word in words:  \n",
    "        for other in words:\n",
    "            vec = get_vector(word, other)\n",
    "            vocab.add_word(word, other, vec)\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550152/550152 [02:46<00:00, 3294.56it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add_word() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5347d7d98c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-4e1520b03a91>\u001b[0m in \u001b[0;36mprepare_vocab\u001b[0;34m(dataset, threshold)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_word() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "vocab = prepare_vocab(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26715"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare wordnet relation vocab\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab('ff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
