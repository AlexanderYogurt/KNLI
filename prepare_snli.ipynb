{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torchnlp.datasets import snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = snli_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = snli_dataset(dev=True)\n",
    "test_data = snli_dataset(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    \n",
    "    output = []\n",
    "    stats = {}\n",
    "    # counts of each class\n",
    "    count_E = 0\n",
    "    count_C = 0\n",
    "    count_N = 0\n",
    "    # lengths of sentences\n",
    "    l_E = {'premise':[], 'hypothesis':[]}\n",
    "    l_C = {'premise':[], 'hypothesis':[]}\n",
    "    l_N = {'premise':[], 'hypothesis':[]}\n",
    "    \n",
    "    for i, t in enumerate(dataset, 0):\n",
    "        \n",
    "        premise = t['premise']\n",
    "        hypothesis = t['hypothesis']\n",
    "        premise_tokens = nltk.word_tokenize(premise)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "        \n",
    "        t['premise_tokens'] = premise_tokens\n",
    "        t['hypothesis_tokens'] = hypothesis_tokens\n",
    "        \n",
    "        if t['label'] == 'neutral':\n",
    "            count_N += 1\n",
    "            l_N['premise'].append(len(premise_tokens))\n",
    "            l_N['hypothesis'].append(len(hypothesis_tokens))\n",
    "        elif t['label'] == 'contradiction':\n",
    "            count_C += 1\n",
    "            l_C['premise'].append(len(premise_tokens))\n",
    "            l_C['hypothesis'].append(len(hypothesis_tokens))\n",
    "        elif t['label'] == 'entailment':\n",
    "            count_E += 1\n",
    "            l_E['premise'].append(len(premise_tokens))\n",
    "            l_E['hypothesis'].append(len(hypothesis_tokens))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        del t['premise_transitions'], t['hypothesis_transitions']\n",
    "        \n",
    "        output.append(t)\n",
    "        \n",
    "    return count_E, count_C, count_N, l_E, l_C, l_N, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_e, tr_c, tr_n, tr_le, tr_lc, tr_ln, train_data = prepare_dataset(train_data)\n",
    "dev_e, dev_c, dev_n, dev_le, dev_lc, dev_ln, dev_data = prepare_dataset(dev_data)\n",
    "test_e, test_c, test_n, test_le, test_lc, test_ln, test_data = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': list(train_data), 'dev': list(dev_data), 'test': list(test_data), \n",
    "        'n_entail': {'train': tr_e, 'dev':dev_e, 'test':test_e},\n",
    "        'n_contradiction': {'train':tr_c, 'dev':dev_c, 'test':test_c}, \n",
    "        'n_neutral': {'train':tr_n, 'dev':dev_n, 'test':test_n}, \n",
    "        'len_entail': {'train': tr_le, 'dev':dev_le, 'test':test_le},\n",
    "        'len_contradiction': {'train':tr_lc, 'dev':dev_lc, 'test':test_lc}, \n",
    "        'len_neutral': {'train':tr_ln, 'dev':dev_ln, 'test':test_ln}, \n",
    "        'split_size': {'train':tr_e + tr_c + tr_n, 'dev':dev_e+dev_c+dev_n, 'test':test_e+test_c+test_n}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open(os.path.join('data', 'snli_data.json'), 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 183416, 'dev': 3329, 'test': 3368}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_entail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 183187, 'dev': 3278, 'test': 3237}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_contradiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 182764, 'dev': 3235, 'test': 3219}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.02853622366642"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['len_entail']['train']['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.971090923679299"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data['len_entail']['train']['premise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.452048894316745"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(data['len_entail']['train']['hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8489398118916593"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data['len_entail']['train']['hypothesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7.45 + 2*2.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.97"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14.03 + 2*5.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.utilities import timeSince, dump_to_json, create_dir, Preload_embedding, read_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d673cc46ad21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "aa, bb = torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An',\n",
       " 'old',\n",
       " 'woman',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sunlit',\n",
       " 'room',\n",
       " 'winds',\n",
       " 'rough',\n",
       " 'yarn',\n",
       " 'into',\n",
       " 'balls',\n",
       " ',',\n",
       " 'the',\n",
       " 'finished',\n",
       " 'yarn',\n",
       " 'balls',\n",
       " 'placed',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pile',\n",
       " 'to',\n",
       " 'her',\n",
       " 'right',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('An old woman in a sunlit room winds rough yarn into balls, the finished yarn balls placed in a pile to her right.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
