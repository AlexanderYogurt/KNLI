{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torchnlp.datasets import snli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = snli_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = snli_dataset(dev=True)\n",
    "test_data = snli_dataset(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    \n",
    "    output = []\n",
    "    stats = {}\n",
    "    count_E = 0\n",
    "    count_C = 0\n",
    "    count_N = 0\n",
    "    l_E = {'premise':[], 'hypothesis':[]}\n",
    "    l_C = {'premise':[], 'hypothesis':[]}\n",
    "    l_N = {'premise':[], 'hypothesis':[]}\n",
    "    \n",
    "    for i, t in enumerate(dataset, 0):\n",
    "        \n",
    "        premise = t['premise']\n",
    "        hypothesis = t['hypothesis']\n",
    "        premise_tokens = nltk.word_tokenize(premise)\n",
    "        hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "        \n",
    "        t['premise_tokens'] = premise_tokens\n",
    "        t['hypothesis_tokens'] = hypothesis_tokens\n",
    "        \n",
    "        if t['label'] == 'neutral':\n",
    "            count_N += 1\n",
    "            l_N['premise'].append(len(premise_tokens))\n",
    "            l_N['hypothesis'].append(len(hypothesis_tokens))\n",
    "        elif t['label'] == 'contradiction':\n",
    "            count_C += 1\n",
    "            l_C['premise'].append(len(premise_tokens))\n",
    "            l_C['hypothesis'].append(len(hypothesis_tokens))\n",
    "        elif t['label'] == 'entailment':\n",
    "            count_E += 1\n",
    "            l_E['premise'].append(len(premise_tokens))\n",
    "            l_E['hypothesis'].append(len(hypothesis_tokens))\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        del t['premise_transitions'], t['hypothesis_transitions']\n",
    "        \n",
    "        output.append(t)\n",
    "        \n",
    "    return count_E, count_C, count_N, l_E, l_C, l_N, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_e, tr_c, tr_n, tr_le, tr_lc, tr_ln, train_data = prepare_dataset(train_data)\n",
    "dev_e, dev_c, dev_n, dev_le, dev_lc, dev_ln, dev_data = prepare_dataset(dev_data)\n",
    "test_e, test_c, test_n, test_le, test_lc, test_ln, test_data = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': list(train_data), 'dev': list(dev_data), 'test': list(test_data), \n",
    "        'n_entail': {'train': tr_e, 'dev':dev_e, 'test':test_e},\n",
    "        'n_contradiction': {'train':tr_c, 'dev':dev_c, 'test':test_c}, \n",
    "        'n_neutral': {'train':tr_n, 'dev':dev_n, 'test':test_n}, \n",
    "        'split_size': {'train':tr_e + tr_c + tr_n, 'dev':dev_e+dev_c+dev_n, 'test':test_e+test_c+test_n}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open(os.path.join('data', 'snli_data.json'), 'w') as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 184201, 'dev': 3487, 'test': 3544}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_entail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 183187, 'dev': 3278, 'test': 3237}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_contradiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 182764, 'dev': 3235, 'test': 3219}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['n_neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in train_data:\n",
    "    if t['label'] == '-':\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.utilities import timeSince, dump_to_json, create_dir, Preload_embedding, read_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d673cc46ad21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "aa, bb = torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
