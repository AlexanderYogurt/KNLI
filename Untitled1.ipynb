{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '$',\n",
       " '1',\n",
       " 'ss',\n",
       " '-',\n",
       " 'ss']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('A person on a horse jumps over a broken down airplane $1 ss-ss')\n",
    "[w.text for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "tokenizer = English()\n",
    "texts = [u\"One document.\", u\"...\", u\"Lots of documents\"]\n",
    "for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lots"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.word_tokenize('A person on a horse jumps over a broken down airplane.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = jsonlines.open('./data/snli/snli_1.0/snli_1.0_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in reader.iter():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['neutral'],\n",
       " 'captionID': '2267923837.jpg#2',\n",
       " 'gold_label': 'neutral',\n",
       " 'pairID': '2267923837.jpg#2r1n',\n",
       " 'sentence1': 'Children smiling and waving at camera',\n",
       " 'sentence1_binary_parse': '( Children ( ( ( smiling and ) waving ) ( at camera ) ) )',\n",
       " 'sentence1_parse': '(ROOT (NP (S (NP (NNP Children)) (VP (VBG smiling) (CC and) (VBG waving) (PP (IN at) (NP (NN camera)))))))',\n",
       " 'sentence2': 'They are smiling at their parents',\n",
       " 'sentence2_binary_parse': '( They ( are ( smiling ( at ( their parents ) ) ) ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VBG smiling) (PP (IN at) (NP (PRP$ their) (NNS parents)))))))'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer):\n",
    "    \n",
    "    output = []\n",
    "    stats = {}\n",
    "    # counts of each class\n",
    "    count_E = 0\n",
    "    count_C = 0\n",
    "    count_N = 0\n",
    "    # lengths of sentences\n",
    "    l_E = {'premise':[], 'hypothesis':[]}\n",
    "    l_C = {'premise':[], 'hypothesis':[]}\n",
    "    l_N = {'premise':[], 'hypothesis':[]}\n",
    "    \n",
    "    with jsonlines.open(dataset) as reader:\n",
    "        pbar = tqdm(total=len(reader))\n",
    "        for t in reader:\n",
    "            pbar.update(1)\n",
    "            tmp = {}\n",
    "            premise = t['sentence1']\n",
    "            hypothesis = t['sentence2']\n",
    "            premise_tokens = [w for w in tokenizer(premise)]\n",
    "            hypothesis_tokens = [w for w in tokenizer(hypothesis)]\n",
    "\n",
    "            tmp['premise'] = premise\n",
    "            tmp['hypothesis'] = hypothesis\n",
    "            tmp['premise_tokens'] = premise_tokens\n",
    "            tmp['hypothesis_tokens'] = hypothesis_tokens\n",
    "            tmp['label'] = t['gold_label']\n",
    "            \n",
    "            if t['gold_label'] == 'neutral':\n",
    "                count_N += 1\n",
    "                l_N['premise'].append(len(premise_tokens))\n",
    "                l_N['hypothesis'].append(len(hypothesis_tokens))\n",
    "            elif t['gold_label'] == 'contradiction':\n",
    "                count_C += 1\n",
    "                l_C['premise'].append(len(premise_tokens))\n",
    "                l_C['hypothesis'].append(len(hypothesis_tokens))\n",
    "            elif t['gold_label'] == 'entailment':\n",
    "                count_E += 1\n",
    "                l_E['premise'].append(len(premise_tokens))\n",
    "                l_E['hypothesis'].append(len(hypothesis_tokens))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            output.append(tmp)\n",
    "        \n",
    "    return count_E, count_C, count_N, l_E, l_C, l_N, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Reader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-2fb8aad2e7ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_le\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_ln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/snli/snli_1.0/snli_1.0_train.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_le\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_ln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/snli/snli_1.0/snli_1.0_dev.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_le\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/snli/snli_1.0/snli_1.0_test.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-dc9287519852>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(dataset, tokenizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Reader' has no len()"
     ]
    }
   ],
   "source": [
    "tr_e, tr_c, tr_n, tr_le, tr_lc, tr_ln, train_data = prepare_dataset('./data/snli/snli_1.0/snli_1.0_train.jsonl', nlp)\n",
    "dev_e, dev_c, dev_n, dev_le, dev_lc, dev_ln, dev_data = prepare_dataset('./data/snli/snli_1.0/snli_1.0_dev.jsonl', nlp)\n",
    "test_e, test_c, test_n, test_le, test_lc, test_ln, test_data = prepare_dataset('./data/snli/snli_1.0/snli_1.0_test.jsonl', nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'train': list(train_data), 'dev': list(dev_data), 'test': list(test_data), \n",
    "        'n_entail': {'train': tr_e, 'dev':dev_e, 'test':test_e},\n",
    "        'n_contradiction': {'train':tr_c, 'dev':dev_c, 'test':test_c}, \n",
    "        'n_neutral': {'train':tr_n, 'dev':dev_n, 'test':test_n}, \n",
    "        'len_entail': {'train': tr_le, 'dev':dev_le, 'test':test_le},\n",
    "        'len_contradiction': {'train':tr_lc, 'dev':dev_lc, 'test':test_lc}, \n",
    "        'len_neutral': {'train':tr_ln, 'dev':dev_ln, 'test':test_ln}, \n",
    "        'split_size': {'train':tr_e + tr_c + tr_n, 'dev':dev_e+dev_c+dev_n, 'test':test_e+test_c+test_n}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
